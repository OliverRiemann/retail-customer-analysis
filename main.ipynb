{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a60f956",
   "metadata": {},
   "source": [
    "# Customer retail notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbd23392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======>                                                   (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|        Country|\n",
      "+---------------+\n",
      "|         Sweden|\n",
      "|      Singapore|\n",
      "|        Germany|\n",
      "|         France|\n",
      "|         Greece|\n",
      "|        Belgium|\n",
      "|        Finland|\n",
      "|          Italy|\n",
      "|           EIRE|\n",
      "|      Lithuania|\n",
      "|         Norway|\n",
      "|          Spain|\n",
      "|        Denmark|\n",
      "|      Hong Kong|\n",
      "|        Iceland|\n",
      "|         Israel|\n",
      "|Channel Islands|\n",
      "|         Cyprus|\n",
      "|    Switzerland|\n",
      "|        Lebanon|\n",
      "+---------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 01_data_ingestion.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"online_retail.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "df.select(\"Country\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c83ae89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "392732"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 02_data_cleaning.py\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Drop nulls\n",
    "df_clean = df.dropna(subset=[\"InvoiceNo\", \"CustomerID\", \"InvoiceDate\", \"UnitPrice\", \"Quantity\"])\n",
    "\n",
    "# Filter out refunds/negative quantities\n",
    "df_clean = df_clean.filter(col(\"Quantity\") > 0)\n",
    "\n",
    "# Remove duplicates\n",
    "df_clean = df_clean.dropDuplicates()\n",
    "\n",
    "df_clean.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "000446bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:==================================================>   (187 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+-----------------+\n",
      "|CustomerID|Recency|Frequency|         Monetary|\n",
      "+----------+-------+---------+-----------------+\n",
      "|   17884.0|      3|      113|695.0699999999999|\n",
      "|   13121.0|    269|       50|283.7300000000001|\n",
      "|   12550.0|     79|       57|964.8299999999996|\n",
      "|   15898.0|      1|       84|          1383.83|\n",
      "|   17392.0|    306|       56|           417.27|\n",
      "+----------+-------+---------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 03_rfm_segmentation.py\n",
    "\n",
    "from pyspark.sql.functions import max, datediff, sum as _sum, count as _count\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "snapshot_date = df_clean.agg(max(\"InvoiceDate\")).collect()[0][0]\n",
    "\n",
    "rfm = df_clean.groupBy(\"CustomerID\").agg(\n",
    "    datediff(lit(snapshot_date), max(\"InvoiceDate\")).alias(\"Recency\"),\n",
    "    _count(\"InvoiceNo\").alias(\"Frequency\"),\n",
    "    _sum(col(\"UnitPrice\") * col(\"Quantity\")).alias(\"Monetary\")\n",
    ")\n",
    "\n",
    "rfm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbbc4941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/27 16:30:07 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "[Stage 230:==================================================>  (191 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+\n",
      "|Churn|prediction|probability|\n",
      "+-----+----------+-----------+\n",
      "|    0|       0.0|  [1.0,0.0]|\n",
      "|    0|       0.0|  [1.0,0.0]|\n",
      "|    0|       0.0|  [1.0,0.0]|\n",
      "|    1|       1.0|  [0.0,1.0]|\n",
      "|    0|       0.0|  [1.0,0.0]|\n",
      "|    1|       1.0|  [0.0,1.0]|\n",
      "|    0|       0.0|  [1.0,0.0]|\n",
      "|    0|       0.0|  [1.0,0.0]|\n",
      "|    1|       1.0|  [0.0,1.0]|\n",
      "|    0|       0.0|  [1.0,0.0]|\n",
      "+-----+----------+-----------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 04_churn_prediction.py\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create churn label (example: customers with no purchases in last N days)\n",
    "rfm = rfm.withColumn(\"Churn\", (rfm[\"Recency\"] > 90).cast(\"integer\"))\n",
    "\n",
    "features = [\"Recency\", \"Frequency\", \"Monetary\"]\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Churn\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "train, test = rfm.randomSplit([0.8, 0.2])\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "predictions.select(\"Churn\", \"prediction\", \"probability\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
